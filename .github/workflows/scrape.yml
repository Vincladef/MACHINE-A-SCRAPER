name: Scrape Emails

on:
  workflow_dispatch:
  # Décommente pour planifier (ex.: tous les jours à 06:00 UTC)
  # schedule:
  #   - cron: "0 6 * * *"

jobs:
  run:
    runs-on: ubuntu-latest
    env:
      SCRAPER_MODE: "scrape_emails"
      CSE_API_KEY: ${{ secrets.CSE_API_KEY }}
      CSE_CX_ID: ${{ secrets.CSE_CX_ID }}
      GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
      GOOGLE_CREDENTIALS: ${{ secrets.GOOGLE_CREDENTIALS }}
      INPUT_SHEET_NAME: "Feuille 1"
      SITES_SHEET_NAME: "Feuille 2"
      EMAILS_SHEET_NAME: "Feuille 3"
      DEEP_SCRAPE: "true"
      MAX_RESULTS: "10"
      REQUEST_DELAY: "1.0"
      HTTP_TIMEOUT: "10"
      APPEND_MODE: "false"
      CSE_LR: "lang_fr"
      CSE_GL: "fr"
      CSE_CR: "countryFR"
      ALLOW_TLDS: "fr"
      EXTRA_QUERY: "site officiel contact email"
      EXCLUDE_TERMS: "reddit upwork linkedin indeed pinterest"
      SKIP_SUBS: "blog.,docs.,help.,support.,news.,jobs."
      TARGET_EMAIL_COUNT: "100"
      MAX_ITERATIONS: "12"
      FALLBACK_TLDS: "com,net,org,io,co,eu"
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run scraper
        run: |
          python scraper.py

